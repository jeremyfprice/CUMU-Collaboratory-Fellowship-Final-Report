@article{southEffectiveUseLikert2022,
  title = {Effective {{Use}} of {{Likert Scales}} in {{Visualization Evaluations}}: {{A Systematic Review}}},
  shorttitle = {Effective {{Use}} of {{Likert Scales}} in {{Visualization Evaluations}}},
  author = {South, Laura and Saffo, David and Vitek, Olga and Dunne, Cody and Borkin, Michelle A.},
  date = {2022},
  journaltitle = {Computer Graphics Forum},
  volume = {41},
  number = {3},
  pages = {43--55},
  issn = {1467-8659},
  doi = {10.1111/cgf.14521},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14521},
  urldate = {2024-08-20},
  abstract = {Likert scales are often used in visualization evaluations to produce quantitative estimates of subjective attributes, such as ease of use or aesthetic appeal. However, the methods used to collect, analyze, and visualize data collected with Likert scales are inconsistent among evaluations in visualization papers. In this paper, we examine the use of Likert scales as a tool for measuring subjective response in a systematic review of 134 visualization evaluations published between 2009 and 2019. We find that papers with both objective and subjective measures do not hold the same reporting and analysis standards for both aspects of their evaluation, producing less rigorous work for the subjective qualities measured by Likert scales. Additionally, we demonstrate that many papers are inconsistent in their interpretations of Likert data as discrete or continuous and may even sacrifice statistical power by applying nonparametric tests unnecessarily. Finally, we identify instances where key details about Likert item construction with the potential to bias participant responses are omitted from evaluation methodology reporting, inhibiting the feasibility and reliability of future replication studies. We summarize recommendations from other fields for best practices with Likert data in visualization evaluations, based on the results of our survey. A full copy of this paper and all supplementary material are available at https://osf.io/exbz8/.},
  langid = {english},
  keywords = {• Human-centered computing → Visualization design and evaluation methods,CCS Concepts,Empirical studies in visualization},
  file = {/Users/jfprice/Zotero/storage/RPNVGX2C/South et al_2022_Effective Use of Likert Scales in Visualization Evaluations.pdf}
}

@article{moerbeekBayesianUpdatingIncreasing2021,
  title = {Bayesian Updating: Increasing Sample Size during the Course of a Study},
  shorttitle = {Bayesian Updating},
  author = {Moerbeek, Mirjam},
  date = {2021-07-05},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Med Res Methodol},
  volume = {21},
  number = {1},
  pages = {137},
  issn = {1471-2288},
  doi = {10.1186/s12874-021-01334-6},
  url = {https://doi.org/10.1186/s12874-021-01334-6},
  urldate = {2024-08-20},
  abstract = {A priori sample size calculation requires an a priori estimate of the size of the effect. An incorrect estimate may result in a sample size that is too low to detect effects or that is unnecessarily high. An alternative to a priori sample size calculation is Bayesian updating, a procedure that allows increasing sample size during the course of a study until sufficient support for a hypothesis is achieved. This procedure does not require and a priori estimate of the effect size. This paper introduces Bayesian updating to researchers in the biomedical field and presents a simulation study that gives insight in sample sizes that may be expected for two-group comparisons.},
  langid = {english},
  keywords = {Bayes factor,Error rate,Informative hypothesis testing},
  file = {/Users/jfprice/Zotero/storage/MRMIANIR/Moerbeek_2021_Bayesian updating.pdf}
}

@article{vanderhornBayesianModelUpdating2018,
  title = {Bayesian Model Updating with Summarized Statistical and Reliability Data},
  author = {VanDerHorn, Eric and Mahadevan, Sankaran},
  date = {2018-04},
  journaltitle = {Reliability Engineering \& System Safety},
  shortjournal = {Reliability Engineering \& System Safety},
  volume = {172},
  pages = {12--24},
  issn = {09518320},
  doi = {10.1016/j.ress.2017.11.023},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0951832017303903},
  urldate = {2024-08-20},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/A3T4ICCK/VanDerHorn_Mahadevan_2018_Bayesian model updating with summarized statistical and reliability data.pdf}
}

@article{wallersteinCommunityBasedParticipatoryResearch2010,
  title = {Community-{{Based Participatory Research Contributions}} to {{Intervention Research}}: {{The Intersection}} of {{Science}} and {{Practice}} to {{Improve Health Equity}}},
  shorttitle = {Community-{{Based Participatory Research Contributions}} to {{Intervention Research}}},
  author = {Wallerstein, Nina and Duran, Bonnie},
  date = {2010-04},
  journaltitle = {American Journal of Public Health},
  shortjournal = {Am J Public Health},
  volume = {100},
  number = {S1},
  pages = {S40-S46},
  issn = {0090-0036, 1541-0048},
  doi = {10.2105/AJPH.2009.184036},
  url = {https://ajph.aphapublications.org/doi/full/10.2105/AJPH.2009.184036},
  urldate = {2024-08-20},
  abstract = {Community-based participatory research (CBPR) has emerged in the last decades as a transformative research paradigm that bridges the gap between science and practice through community engagement and social action to increase health equity.             CBPR expands the potential for the translational sciences to develop, implement, and disseminate effective interventions across diverse communities through strategies to redress power imbalances; facilitate mutual benefit among community and academic partners; and promote reciprocal knowledge translation, incorporating community theories into the research.             We identify the barriers and challenges within the intervention and implementation sciences, discuss how CBPR can address these challenges, provide an illustrative research example, and discuss next steps to advance the translational science of CBPR.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/5LYJHLG4/Wallerstein_Duran_2010_Community-Based Participatory Research Contributions to Intervention Research.pdf}
}

@article{wallersteinWhatPredictsOutcomes2008,
  title = {What Predicts Outcomes in {{CBPR}}},
  author = {Wallerstein, Nina and Oetzel, John and Duran, Bonnie and Tafoya, Greg and Belone, Lorenda and Rae, Rebecca},
  date = {2008},
  journaltitle = {Community-based participatory research for health: From process to outcomes},
  volume = {2},
  pages = {371--92},
  publisher = {Jossey-Bass San Francisco},
  url = {https://www.researchgate.net/profile/Nina-Wallerstein/publication/285496812_What_predicts_outcomes_in_CBPR/links/5663556408ae15e746313863/What-predicts-outcomes-in-CBPR.pdf},
  urldate = {2024-08-20},
  keywords = {⛔ No DOI found},
  file = {/Users/jfprice/Zotero/storage/9IN4EWE7/Wallerstein et al_2008_What predicts outcomes in CBPR.pdf}
}

@online{robertsonQTIPQMethodTesting,
  title = {Q-{{TIP}}: {{Q-Method Testing}} and {{Inquiry Platform}}},
  author = {Robertson, Morgan and Nost, Eric and Lave, Rebecca},
  url = {https://qtip.geography.wisc.edu/#/},
  urldate = {2024-08-19},
  organization = {Q-TIP: Q-Method Testing and Inquiry Platform},
  file = {/Users/jfprice/Zotero/storage/6JIW76UI/qtip.geography.wisc.edu.html}
}

@article{banasickKADEDesktopApplication2019,
  title = {{{KADE}}: {{A}} Desktop Application for {{Q}} Methodology},
  shorttitle = {{{KADE}}},
  author = {Banasick, Shawn},
  date = {2019-04-23},
  journaltitle = {Journal of Open Source Software},
  shortjournal = {JOSS},
  volume = {4},
  number = {36},
  pages = {1360},
  issn = {2475-9066},
  doi = {10.21105/joss.01360},
  url = {http://joss.theoj.org/papers/10.21105/joss.01360},
  urldate = {2024-07-21},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/HWSIV88Y/Banasick_2019_KADE.pdf}
}

@article{gatesValuingCriticalSystems2018,
  title = {Toward {{Valuing With Critical Systems Heuristics}}},
  author = {Gates, Emily F.},
  date = {2018-06-01},
  journaltitle = {American Journal of Evaluation},
  volume = {39},
  number = {2},
  pages = {201--220},
  publisher = {SAGE Publications Inc},
  issn = {1098-2140},
  doi = {10.1177/1098214017703703},
  url = {https://doi.org/10.1177/1098214017703703},
  urldate = {2024-07-14},
  abstract = {Evaluation is defined by its central task of valuing—the process and product of judging the merit, worth, or significance of a policy or program. However, there are no clear-cut ways to consider values and render value judgments in evaluation practice. There remains contention in the evaluation field about whether and how to make value judgments. No approach to valuing eliminates the uncertainty, plurality, and potential for conflict that comes with considering values. This article explores what critical systems heuristics (CSH), an area of applied systems thinking, might contribute to four long-standing issues regarding valuing: envisioning the social value of evaluation, framing the evaluand and evaluation, selecting and justifying criteria, and determining the roles of the evaluator(s) and stakeholders in valuing. CSH contributes concepts and tools that, in theory, support more reflective, responsible valuing although further practical application is needed.},
  langid = {english}
}

@article{wallersteinEngageEquityLongTerm2020,
  title = {Engage for {{Equity}}: {{A Long-Term Study}} of {{Community-Based Participatory Research}} and {{Community-Engaged Research Practices}} and {{Outcomes}}},
  shorttitle = {Engage for {{Equity}}},
  author = {Wallerstein, Nina and Oetzel, John G. and Sanchez-Youngman, Shannon and Boursaw, Blake and Dickson, Elizabeth and Kastelic, Sarah and Koegel, Paul and Lucero, Julie E. and Magarati, Maya and Ortiz, Kasim and Parker, Myra and Peña, Juan and Richmond, Alan and Duran, Bonnie},
  date = {2020-06-01},
  journaltitle = {Health Education \& Behavior},
  shortjournal = {Health Educ Behav},
  volume = {47},
  number = {3},
  pages = {380--390},
  publisher = {SAGE Publications Inc},
  issn = {1090-1981},
  doi = {10.1177/1090198119897075},
  url = {https://doi.org/10.1177/1090198119897075},
  urldate = {2024-03-16},
  abstract = {Community-based participatory research (CBPR) and community-engaged research have been established in the past 25 years as valued research approaches within health education, public health, and other health and social sciences for their effectiveness in reducing inequities. While early literature focused on partnering principles and processes, within the past decade, individual studies, as well as systematic reviews, have increasingly documented outcomes in community support and empowerment, sustained partnerships, healthier behaviors, policy changes, and health improvements. Despite enhanced focus on research and health outcomes, the science lags behind the practice. CBPR partnering pathways that result in outcomes remain little understood, with few studies documenting best practices. Since 2006, the University of New Mexico Center for Participatory Research with the University of Washington’s Indigenous Wellness Research Institute and partners across the country has engaged in targeted investigations to fill this gap in the science. Our inquiry, spanning three stages of National Institutes of Health funding, has sought to identify which partnering practices, under which contexts and conditions, have capacity to contribute to health, research, and community outcomes. This article presents the research design of our current grant, Engage for Equity, including its history, social justice principles, theoretical bases, measures, intervention tools and resources, and preliminary findings about collective empowerment as our middle range theory of change. We end with lessons learned and recommendations for partnerships to engage in collective reflexive practice to strengthen internal power-sharing and capacity to reach health and social equity outcomes.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/T8C3QUW5/Wallerstein et al_2020_Engage for Equity.pdf}
}

@article{boursawScalesPracticesOutcomes2021,
  title = {Scales of {{Practices}} and {{Outcomes}} for {{Community-Engaged Research}}},
  author = {Boursaw, Blake and Oetzel, John G. and Dickson, Elizabeth and Thein, Thomas S. and Sanchez-Youngman, Shannon and Peña, Juan and Parker, Myra and Magarati, Maya and Littledeer, Lenora and Duran, Bonnie and Wallerstein, Nina},
  date = {2021},
  journaltitle = {American Journal of Community Psychology},
  volume = {67},
  number = {3-4},
  pages = {256--270},
  issn = {1573-2770},
  doi = {10.1002/ajcp.12503},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ajcp.12503},
  urldate = {2024-03-16},
  abstract = {Despite the growth of research on community-engaged research (CEnR), recent reviews suggest there has been limited development of validated scales to measure key contexts, mechanisms, and outcomes, impairing testing and refinement of theoretical models. The purpose of this study is to present the psychometric properties of scales from the Engage for Equity (E2) project, stemming from a long-term research partnership examining community-engaged research projects. This study used a three-stage, cross-sectional format: (a) a sampling frame of 413 CEnR projects was identified; (b) 210 principal investigators completed a project-level survey and nominated partners for another survey; (c) 457 investigators and partners completed a survey about project contexts, processes, interventions, and outcomes. Factorial validity was established through confirmatory factor analysis supporting seven scales: contextual capacity, commitment to collective empowerment, relationships, community engagement in research actions, synergy, partner and partnership transformation, and projected outcomes. Convergent validity was established through examining covariances among the scales. This study largely yielded results consistent with a previous psychometric study of related measures, while demonstrating improved ceiling effects of the items and refined conceptualization of core theoretical constructs.},
  langid = {english},
  keywords = {Community-based participatory research,Community-engaged research,Empowerment,Measurement,Psychometrics},
  file = {/Users/jfprice/Zotero/storage/LMVEMPK9/Boursaw et al_2021_Scales of Practices and Outcomes for Community-Engaged Research.pdf;/Users/jfprice/Zotero/storage/BQWQUXIY/ajcp.html}
}

@report{oregondepartmentofenvironmentalqualityEnhancingCommunityEngagement2022,
  title = {Enhancing {{Community Engagement Using Q-Methodology}}},
  author = {Oregon Department of Environmental Quality},
  date = {2022},
  institution = {Oregon Department of Environmental Quality},
  location = {Portland, OR},
  url = {https://www.oregon.gov/deq/mm/Documents/recQmethodRep.pdf},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/Users/jfprice/Zotero/storage/AEEF4DDU/Oregon Department of Environmental Quality_2022_Enhancing Community Engagement Using Q-Methodology.pdf}
}

@article{morrisonExploringFacultyPerspectives2017,
  title = {Exploring {{Faculty Perspectives}} on {{Community Engaged Scholarship}}: {{The Case}} for {{Q Methodology}}},
  shorttitle = {Exploring {{Faculty Perspectives}} on {{Community Engaged Scholarship}}},
  author = {Morrison, Emily and Wagner, Wendy},
  date = {2017-02-22},
  journaltitle = {Michigan Journal of Community Service Learning},
  volume = {23},
  number = {1},
  issn = {1944-0219},
  doi = {10.3998/mjcsloa.3239521.0023.101},
  url = {http://hdl.handle.net/2027/spo.3239521.0023.101},
  urldate = {2023-11-01},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/UDKY74VS/Morrison_Wagner_2017_Exploring Faculty Perspectives on Community Engaged Scholarship.pdf}
}

@article{mukherjeeComparisonTechniquesEliciting2018,
  title = {Comparison of Techniques for Eliciting Views and Judgements in Decision-Making},
  author = {Mukherjee, Nibedita and Zabala, Aiora and Huge, Jean and Nyumba, Tobias Ochieng and Adem Esmail, Blal and Sutherland, William J.},
  date = {2018},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {9},
  number = {1},
  pages = {54--63},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12940},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12940},
  urldate = {2023-10-20},
  abstract = {Decision-making is a complex process that typically includes a series of stages: identifying the issue, considering possible options, making judgements and then making a decision by combining information and values. The current status quo relies heavily on the informational aspect of decision-making with little or no emphasis on the value positions that affect decisions. There is increasing realization of the importance of adopting rigorous methods for each stage such that the information, views and judgements of stakeholders and experts are used in a systematic and repeatable manner. Though there are several methodological textbooks which discuss a plethora of social science techniques, it is hard to judge the suitability of any given technique for a given decision problem. In decision-making, the three critical aspects are “what” decision is to be made, “who” makes the decisions and “how” the decisions are made. The methods covered in this paper focus on “how” decisions can be made. We compare six techniques: Focus Group Discussion (FGD), Interviews, Q methodology, Multi-criteria Decision Analysis (MCDA), Nominal Group Technique and the Delphi technique specifically in the context of biodiversity conservation. All of these techniques (with the exception of MCDA) help in understanding human values and the underlying perspectives which shape decisions. Based on structured reviews of 423 papers covering all six methods, we compare the conceptual and logistical characteristics of the methods, and map their suitability for the different stages of the decision-making process. While interviews and FGD are well-known, techniques such the Nominal Group technique and Q methodology are relatively under-used. In situations where conflict is high, we recommend using the Q methodology and Delphi technique to elicit judgements. Where conflict is low, and a consensus is needed urgently, the Nominal Group technique may be more suitable. We present a nuanced synthesis of methods aimed at users. The comparison of the different techniques might be useful for project managers, academics or practitioners in the planning phases of their projects and help in making better informed methodological choices.},
  langid = {english},
  keywords = {conservation,decision-making,Delphi technique,focus group discussion,interview,multi-criteria decision,nominal group technique,Q methodology},
  file = {/Users/jfprice/Zotero/storage/7HFV2NC4/Mukherjee et al_2018_Comparison of techniques for eliciting views and judgements in decision-making.pdf;/Users/jfprice/Zotero/storage/NP32SHFR/2041-210X.html}
}

@article{beloneCommunityBasedParticipatoryResearch2016a,
  title = {Community-{{Based Participatory Research Conceptual Model}}: {{Community Partner Consultation}} and {{Face Validity}}},
  shorttitle = {Community-{{Based Participatory Research Conceptual Model}}},
  author = {Belone, Lorenda and family=Lucero, given=JE., given-i={{JE}} and Duran, B. and Tafoya, G. and family=Baker, given=EA., given-i={{EA}} and Chan, D. and Chang, C. and Greene-Moton, E. and Kelley, M. and Wallerstein, Nina},
  date = {2016-01},
  journaltitle = {Qualitative health research},
  shortjournal = {Qual Health Res},
  volume = {26},
  number = {1},
  eprint = {25361792},
  eprinttype = {pmid},
  pages = {117--135},
  issn = {1049-7323},
  doi = {10.1177/1049732314557084},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839192/},
  urldate = {2023-10-18},
  abstract = {A national community based participatory research (CBPR) team developed a conceptual/logic model of CBPR partnerships to understand the contribution of partnership processes to improved community capacity and health outcomes. With the model primarily developed through academic literature and expert consensus-building, we sought community input to assess face validity and acceptability. Our research team conducted semi-structured focus groups with six partnerships nation-wide. Participants validated and expanded upon existing model constructs and identified new constructs based on “real-world” praxis, resulting in a revised model. Four cross-cutting constructs were identified: trust development, capacity, mutual learning, and power dynamics. By empirically testing the model, we found community face validity and capacity to adapt the model to diverse contexts. We recommend partnerships use and adapt the CBPR model and its constructs, for collective reflection and evaluation, to enhance their partnering practices and achieve their health and research goals.},
  pmcid = {PMC4839192},
  file = {/Users/jfprice/Zotero/storage/KNRED9LT/Belone et al_2016_Community-Based Participatory Research Conceptual Model.pdf}
}

@article{moreno-boteHeuristicsOptimalSolutions2020,
  title = {Heuristics and Optimal Solutions to the Breadth–Depth Dilemma},
  author = {Moreno-Bote, Rubén and Ramírez-Ruiz, Jorge and Drugowitsch, Jan and Hayden, Benjamin Y.},
  date = {2020-08-18},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {33},
  pages = {19799--19808},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2004929117},
  url = {https://www.pnas.org/doi/10.1073/pnas.2004929117},
  urldate = {2023-05-12},
  abstract = {In multialternative risky choice, we are often faced with the opportunity to allocate our limited information-gathering capacity between several options before receiving feedback. In such cases, we face a natural trade-off between breadth—spreading our capacity across many options—and depth—gaining more information about a smaller number of options. Despite its broad relevance to daily life, including in many naturalistic foraging situations, the optimal strategy in the breadth–depth trade-off has not been delineated. Here, we formalize the breadth–depth dilemma through a finite-sample capacity model. We find that, if capacity is small (∼10 samples), it is optimal to draw one sample per alternative, favoring breadth. However, for larger capacities, a sharp transition is observed, and it becomes best to deeply sample a very small fraction of alternatives, which roughly decreases with the square root of capacity. Thus, ignoring most options, even when capacity is large enough to shallowly sample all of them, is a signature of optimal behavior. Our results also provide a rich casuistic for metareasoning in multialternative decisions with bounded capacity using close-to-optimal heuristics.},
  file = {/Users/jfprice/Zotero/storage/NHTETDPD/Moreno-Bote et al_2020_Heuristics and optimal solutions to the breadth–depth dilemma.pdf}
}

@online{mcchesneyWhyYouShould2020,
  title = {Why You Should Summarize Your Data with the Geometric Mean},
  author = {McChesney, Jasper},
  date = {2020-10-16T13:30:52},
  url = {https://jlmc.medium.com/understanding-three-simple-statistics-for-data-visualizations-2619dbb3677a},
  urldate = {2024-05-28},
  abstract = {(or two other common statisitcs)},
  langid = {english},
  organization = {Medium},
  file = {/Users/jfprice/Zotero/storage/399FEPU7/understanding-three-simple-statistics-for-data-visualizations-2619dbb3677a.html}
}

@online{mcnicholAverageYouRe2022,
  title = {On {{Average}}, {{You}}’re {{Using}} the {{Wrong Average}}: {{Geometric}} \& {{Harmonic Means}} in {{Data Analysis}}},
  shorttitle = {On {{Average}}, {{You}}’re {{Using}} the {{Wrong Average}}},
  author = {McNichol, Daniel},
  date = {2022-01-24T01:18:13},
  url = {https://towardsdatascience.com/on-average-youre-using-the-wrong-average-geometric-harmonic-means-in-data-analysis-2a703e21ea0},
  urldate = {2024-05-28},
  abstract = {When the Mean Doesn’t Mean What You Think it Means},
  langid = {english},
  organization = {Medium},
  file = {/Users/jfprice/Zotero/storage/6B74IVRX/on-average-youre-using-the-wrong-average-geometric-harmonic-means-in-data-analysis-2a703e21ea0.html}
}

@article{smithSalienceCountsandDoes1997,
  title = {Salience Counts-and so Does Accuracy: {{Correcting}} and Updating a Measure for Free-List-Item Salience},
  shorttitle = {Salience Counts-and so Does Accuracy},
  author = {Smith, J. Jerome and Borgatti, Stephen P.},
  date = {1997},
  journaltitle = {Journal of Linguistic Anthropology},
  volume = {7},
  pages = {208--209},
  doi = {10.1525/jlin.1997.7.2.208},
  url = {http://qualquant.org/wp-content/uploads/cda/*smith%20and%20borgatti%201997%20salience%20and%20accuracy.pdf},
  urldate = {2017-08-23},
  file = {/Users/jfprice/Zotero/storage/3CSG5VAB/smith and borgatti 1997 salience and accuracy.pdf}
}

@software{rcoreteamLanguageEnvironmentStatistical2022,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {R Core Team},
  date = {2022},
  location = {Vienna, Austria},
  url = {https://www.R-project.org/},
  organization = {R Foundation for Statistical Computing}
}

@article{purzyckiAnthroToolsPackageCrossCultural2017a,
  title = {{{AnthroTools}}: {{An R Package}} for {{Cross-Cultural Ethnographic Data Analysis}}},
  shorttitle = {{{AnthroTools}}},
  author = {Purzycki, Benjamin Grant and Jamieson-Lane, Alastair},
  date = {2017-02},
  journaltitle = {Cross-Cultural Research},
  shortjournal = {Cross-Cultural Research},
  volume = {51},
  number = {1},
  pages = {51--74},
  issn = {1069-3971, 1552-3578},
  doi = {10.1177/1069397116680352},
  url = {http://journals.sagepub.com/doi/10.1177/1069397116680352},
  urldate = {2022-06-02},
  abstract = {As large-scale collaborative, cross-cultural ethnographic research becomes easier and easier to realize, certain ethnographic methods and analyses should be correspondingly more available, inviting, and accommodating. We have therefore created AnthroTools, a package for the free, opensource language R, with a variety of tools and functions suitable for both multi-factor free-list analysis and Bayesian cultural consensus modeling. Free-list data elicitation is a simple technique for ethnographic research. However, especially for cross-cultural free-list data, background preparation is considerable and often requires specific software. In addition, although current cultural consensus analysis tools offer very sophisticated analyses, they also either require specialized software or have computationally taxing methods. AnthroTools expedites these techniques, rapidly performs diagnostics, and prepares data for further analysis. In this article, we briefly discuss what this package offers cross-cultural researchers and provide basic examples of some of its functions.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/7XTX34G8/Purzycki_Jamieson-Lane_2017_AnthroTools.pdf}
}

@article{fiksUsingFreelistingUnderstand2011,
  title = {Using Freelisting to Understand Shared Decision Making in {{ADHD}}: {{Parents}}’ and Pediatricians’ Perspectives},
  shorttitle = {Using Freelisting to Understand Shared Decision Making in {{ADHD}}},
  author = {Fiks, Alexander G. and Gafen, Angela and Hughes, Cayce C. and Hunter, Kenya F. and Barg, Frances K.},
  date = {2011-08},
  journaltitle = {Patient education and counseling},
  shortjournal = {Patient Educ Couns},
  volume = {84},
  number = {2},
  eprint = {20797833},
  eprinttype = {pmid},
  pages = {236--244},
  issn = {0738-3991},
  doi = {10.1016/j.pec.2010.07.035},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3551534/},
  urldate = {2024-07-25},
  abstract = {Objective To compare and contrast notions of ADHD among pediatricians and parents of affected children to understand the perspectives they bring to shared decision making (SDM). Methods In this freelisting study, 60 parents of children with ADHD and 30 primary care pediatricians listed words reflecting their understanding of (1) Attention Deficit Hyperactivity Disorder (ADHD), (2) getting/offering help for ADHD, (3) talking to doctors/families about ADHD, and (4) “mental health.” Smith’s salience score established terms that were salient and cultural consensus analysis identified variation within subgroups of participants. Results Parents’ terms reflected ADHD’s effects on the child and family, while clinicians often mentioned school. Lists suggested differing needs and goals for clinicians and subgroups of parents in SDM: “time” for clinicians, “learning” and “understanding” for non-college educated parents, and “comfort” and “relief” for college educated parents. Neither parents nor clinicians framed ADHD in the same way as “mental health.” Conclusion Parents and clinicians, who conceptualize ADHD differently, should negotiate a shared understanding of ADHD as a basis for SDM. Treatment discussions should be tailored to encompass families’ varied emotional and educational needs. Practice implications Fostering SDM in primary care is consonant with notions of ADHD as distinct from mental health.},
  pmcid = {PMC3551534},
  file = {/Users/jfprice/Zotero/storage/L747QLRJ/Fiks et al_2011_Using freelisting to understand shared decision making in ADHD.pdf}
}

@incollection{quinlanFreelistingMethod2018,
  title = {The {{Freelisting Method}}},
  booktitle = {Handbook of {{Research Methods}} in {{Health Social Sciences}}},
  author = {Quinlan, Marsha B.},
  editor = {Liamputtong, Pranee},
  date = {2018},
  pages = {1--16},
  publisher = {Springer Singapore},
  location = {Singapore},
  doi = {10.1007/978-981-10-2779-6_12-2},
  url = {http://link.springer.com/10.1007/978-981-10-2779-6_12-2},
  urldate = {2022-08-19},
  abstract = {A freelist is a mental inventory of items an individual thinks of within a given category. Freelists reveal cultural “salience” of particular notions within groups, and variation in individuals’ topical knowledge across groups. The ease and accuracy of freelist interviewing, or freelisting, makes it ideal for collecting data on health knowledge and beliefs from relatively large samples. Successful freelisting requires researchers to break the research topic into honed categories. Research participants presented with broad prompts tend to “unpack” mental subcategories and may omit (forget) common items or categories. Researchers should find subdomains to present individually for participants to unpack in separate smaller freelists. Researchers may focus the freelist prompts through successive freelisting, pile sorts, or focus group-interviews. Written freelisting among literate populations allows for rapid data collection, possibly from multiple individuals simultaneously. Among nonliterate peoples, using oral freelists remains a relatively rapid method; however, interviewers must prevent bystanders from “contaminating” individual interviewees’ lists. Researchers should cross-check freelist responses with informal methods as much as practicable to contextualize and understand the references therein. With proper attention to detail, freelisting can amass high quality data on people’s medical understanding, attitudes, and behaviors.},
  isbn = {978-981-10-2779-6},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/TXJP43EH/Quinlan_2018_The Freelisting Method.pdf}
}
