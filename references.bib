@article{southEffectiveUseLikert2022,
  title = {Effective {{Use}} of {{Likert Scales}} in {{Visualization Evaluations}}: {{A Systematic Review}}},
  shorttitle = {Effective {{Use}} of {{Likert Scales}} in {{Visualization Evaluations}}},
  author = {South, Laura and Saffo, David and Vitek, Olga and Dunne, Cody and Borkin, Michelle A.},
  date = {2022},
  journaltitle = {Computer Graphics Forum},
  volume = {41},
  number = {3},
  pages = {43--55},
  issn = {1467-8659},
  doi = {10.1111/cgf.14521},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14521},
  urldate = {2024-08-20},
  abstract = {Likert scales are often used in visualization evaluations to produce quantitative estimates of subjective attributes, such as ease of use or aesthetic appeal. However, the methods used to collect, analyze, and visualize data collected with Likert scales are inconsistent among evaluations in visualization papers. In this paper, we examine the use of Likert scales as a tool for measuring subjective response in a systematic review of 134 visualization evaluations published between 2009 and 2019. We find that papers with both objective and subjective measures do not hold the same reporting and analysis standards for both aspects of their evaluation, producing less rigorous work for the subjective qualities measured by Likert scales. Additionally, we demonstrate that many papers are inconsistent in their interpretations of Likert data as discrete or continuous and may even sacrifice statistical power by applying nonparametric tests unnecessarily. Finally, we identify instances where key details about Likert item construction with the potential to bias participant responses are omitted from evaluation methodology reporting, inhibiting the feasibility and reliability of future replication studies. We summarize recommendations from other fields for best practices with Likert data in visualization evaluations, based on the results of our survey. A full copy of this paper and all supplementary material are available at https://osf.io/exbz8/.},
  langid = {english},
  keywords = {• Human-centered computing → Visualization design and evaluation methods,CCS Concepts,Empirical studies in visualization},
  file = {/Users/jfprice/Zotero/storage/RPNVGX2C/South et al_2022_Effective Use of Likert Scales in Visualization Evaluations.pdf}
}

@article{moerbeekBayesianUpdatingIncreasing2021,
  title = {Bayesian Updating: Increasing Sample Size during the Course of a Study},
  shorttitle = {Bayesian Updating},
  author = {Moerbeek, Mirjam},
  date = {2021-07-05},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Med Res Methodol},
  volume = {21},
  number = {1},
  pages = {137},
  issn = {1471-2288},
  doi = {10.1186/s12874-021-01334-6},
  url = {https://doi.org/10.1186/s12874-021-01334-6},
  urldate = {2024-08-20},
  abstract = {A priori sample size calculation requires an a priori estimate of the size of the effect. An incorrect estimate may result in a sample size that is too low to detect effects or that is unnecessarily high. An alternative to a priori sample size calculation is Bayesian updating, a procedure that allows increasing sample size during the course of a study until sufficient support for a hypothesis is achieved. This procedure does not require and a priori estimate of the effect size. This paper introduces Bayesian updating to researchers in the biomedical field and presents a simulation study that gives insight in sample sizes that may be expected for two-group comparisons.},
  langid = {english},
  keywords = {Bayes factor,Error rate,Informative hypothesis testing},
  file = {/Users/jfprice/Zotero/storage/MRMIANIR/Moerbeek_2021_Bayesian updating.pdf}
}

@article{vanderhornBayesianModelUpdating2018,
  title = {Bayesian Model Updating with Summarized Statistical and Reliability Data},
  author = {VanDerHorn, Eric and Mahadevan, Sankaran},
  date = {2018-04},
  journaltitle = {Reliability Engineering \& System Safety},
  shortjournal = {Reliability Engineering \& System Safety},
  volume = {172},
  pages = {12--24},
  issn = {09518320},
  doi = {10.1016/j.ress.2017.11.023},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0951832017303903},
  urldate = {2024-08-20},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/A3T4ICCK/VanDerHorn_Mahadevan_2018_Bayesian model updating with summarized statistical and reliability data.pdf}
}

@article{wallersteinCommunityBasedParticipatoryResearch2010,
  title = {Community-{{Based Participatory Research Contributions}} to {{Intervention Research}}: {{The Intersection}} of {{Science}} and {{Practice}} to {{Improve Health Equity}}},
  shorttitle = {Community-{{Based Participatory Research Contributions}} to {{Intervention Research}}},
  author = {Wallerstein, Nina and Duran, Bonnie},
  date = {2010-04},
  journaltitle = {American Journal of Public Health},
  shortjournal = {Am J Public Health},
  volume = {100},
  number = {S1},
  pages = {S40-S46},
  issn = {0090-0036, 1541-0048},
  doi = {10.2105/AJPH.2009.184036},
  url = {https://ajph.aphapublications.org/doi/full/10.2105/AJPH.2009.184036},
  urldate = {2024-08-20},
  abstract = {Community-based participatory research (CBPR) has emerged in the last decades as a transformative research paradigm that bridges the gap between science and practice through community engagement and social action to increase health equity.             CBPR expands the potential for the translational sciences to develop, implement, and disseminate effective interventions across diverse communities through strategies to redress power imbalances; facilitate mutual benefit among community and academic partners; and promote reciprocal knowledge translation, incorporating community theories into the research.             We identify the barriers and challenges within the intervention and implementation sciences, discuss how CBPR can address these challenges, provide an illustrative research example, and discuss next steps to advance the translational science of CBPR.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/5LYJHLG4/Wallerstein_Duran_2010_Community-Based Participatory Research Contributions to Intervention Research.pdf}
}

@article{wallersteinWhatPredictsOutcomes2008,
  title = {What Predicts Outcomes in {{CBPR}}},
  author = {Wallerstein, Nina and Oetzel, John and Duran, Bonnie and Tafoya, Greg and Belone, Lorenda and Rae, Rebecca},
  date = {2008},
  journaltitle = {Community-based participatory research for health: From process to outcomes},
  volume = {2},
  pages = {371--92},
  publisher = {Jossey-Bass San Francisco},
  url = {https://www.researchgate.net/profile/Nina-Wallerstein/publication/285496812_What_predicts_outcomes_in_CBPR/links/5663556408ae15e746313863/What-predicts-outcomes-in-CBPR.pdf},
  urldate = {2024-08-20},
  keywords = {⛔ No DOI found},
  file = {/Users/jfprice/Zotero/storage/9IN4EWE7/Wallerstein et al_2008_What predicts outcomes in CBPR.pdf}
}

@online{robertsonQTIPQMethodTesting,
  title = {Q-{{TIP}}: {{Q-Method Testing}} and {{Inquiry Platform}}},
  author = {Robertson, Morgan and Nost, Eric and Lave, Rebecca},
  date = {2024},
  url = {https://qtip.geography.wisc.edu/#/},
  urldate = {2024-08-19},
  organization = {Q-TIP: Q-Method Testing and Inquiry Platform},
  file = {/Users/jfprice/Zotero/storage/6JIW76UI/qtip.geography.wisc.edu.html}
}

@article{banasickKADEDesktopApplication2019,
  title = {{{KADE}}: {{A}} Desktop Application for {{Q}} Methodology},
  shorttitle = {{{KADE}}},
  author = {Banasick, Shawn},
  date = {2019-04-23},
  journaltitle = {Journal of Open Source Software},
  shortjournal = {JOSS},
  volume = {4},
  number = {36},
  pages = {1360},
  issn = {2475-9066},
  doi = {10.21105/joss.01360},
  url = {http://joss.theoj.org/papers/10.21105/joss.01360},
  urldate = {2024-07-21},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/HWSIV88Y/Banasick_2019_KADE.pdf}
}

@article{gatesValuingCriticalSystems2018,
  title = {Toward {{Valuing With Critical Systems Heuristics}}},
  author = {Gates, Emily F.},
  date = {2018-06-01},
  journaltitle = {American Journal of Evaluation},
  volume = {39},
  number = {2},
  pages = {201--220},
  publisher = {SAGE Publications Inc},
  issn = {1098-2140},
  doi = {10.1177/1098214017703703},
  url = {https://doi.org/10.1177/1098214017703703},
  urldate = {2024-07-14},
  abstract = {Evaluation is defined by its central task of valuing—the process and product of judging the merit, worth, or significance of a policy or program. However, there are no clear-cut ways to consider values and render value judgments in evaluation practice. There remains contention in the evaluation field about whether and how to make value judgments. No approach to valuing eliminates the uncertainty, plurality, and potential for conflict that comes with considering values. This article explores what critical systems heuristics (CSH), an area of applied systems thinking, might contribute to four long-standing issues regarding valuing: envisioning the social value of evaluation, framing the evaluand and evaluation, selecting and justifying criteria, and determining the roles of the evaluator(s) and stakeholders in valuing. CSH contributes concepts and tools that, in theory, support more reflective, responsible valuing although further practical application is needed.},
  langid = {english}
}

@article{wallersteinEngageEquityLongTerm2020,
  title = {Engage for {{Equity}}: {{A Long-Term Study}} of {{Community-Based Participatory Research}} and {{Community-Engaged Research Practices}} and {{Outcomes}}},
  shorttitle = {Engage for {{Equity}}},
  author = {Wallerstein, Nina and Oetzel, John G. and Sanchez-Youngman, Shannon and Boursaw, Blake and Dickson, Elizabeth and Kastelic, Sarah and Koegel, Paul and Lucero, Julie E. and Magarati, Maya and Ortiz, Kasim and Parker, Myra and Peña, Juan and Richmond, Alan and Duran, Bonnie},
  date = {2020-06-01},
  journaltitle = {Health Education \& Behavior},
  shortjournal = {Health Educ Behav},
  volume = {47},
  number = {3},
  pages = {380--390},
  publisher = {SAGE Publications Inc},
  issn = {1090-1981},
  doi = {10.1177/1090198119897075},
  url = {https://doi.org/10.1177/1090198119897075},
  urldate = {2024-03-16},
  abstract = {Community-based participatory research (CBPR) and community-engaged research have been established in the past 25 years as valued research approaches within health education, public health, and other health and social sciences for their effectiveness in reducing inequities. While early literature focused on partnering principles and processes, within the past decade, individual studies, as well as systematic reviews, have increasingly documented outcomes in community support and empowerment, sustained partnerships, healthier behaviors, policy changes, and health improvements. Despite enhanced focus on research and health outcomes, the science lags behind the practice. CBPR partnering pathways that result in outcomes remain little understood, with few studies documenting best practices. Since 2006, the University of New Mexico Center for Participatory Research with the University of Washington’s Indigenous Wellness Research Institute and partners across the country has engaged in targeted investigations to fill this gap in the science. Our inquiry, spanning three stages of National Institutes of Health funding, has sought to identify which partnering practices, under which contexts and conditions, have capacity to contribute to health, research, and community outcomes. This article presents the research design of our current grant, Engage for Equity, including its history, social justice principles, theoretical bases, measures, intervention tools and resources, and preliminary findings about collective empowerment as our middle range theory of change. We end with lessons learned and recommendations for partnerships to engage in collective reflexive practice to strengthen internal power-sharing and capacity to reach health and social equity outcomes.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/T8C3QUW5/Wallerstein et al_2020_Engage for Equity.pdf}
}

@article{boursawScalesPracticesOutcomes2021,
  title = {Scales of {{Practices}} and {{Outcomes}} for {{Community-Engaged Research}}},
  author = {Boursaw, Blake and Oetzel, John G. and Dickson, Elizabeth and Thein, Thomas S. and Sanchez-Youngman, Shannon and Peña, Juan and Parker, Myra and Magarati, Maya and Littledeer, Lenora and Duran, Bonnie and Wallerstein, Nina},
  date = {2021},
  journaltitle = {American Journal of Community Psychology},
  volume = {67},
  number = {3-4},
  pages = {256--270},
  issn = {1573-2770},
  doi = {10.1002/ajcp.12503},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ajcp.12503},
  urldate = {2024-03-16},
  abstract = {Despite the growth of research on community-engaged research (CEnR), recent reviews suggest there has been limited development of validated scales to measure key contexts, mechanisms, and outcomes, impairing testing and refinement of theoretical models. The purpose of this study is to present the psychometric properties of scales from the Engage for Equity (E2) project, stemming from a long-term research partnership examining community-engaged research projects. This study used a three-stage, cross-sectional format: (a) a sampling frame of 413 CEnR projects was identified; (b) 210 principal investigators completed a project-level survey and nominated partners for another survey; (c) 457 investigators and partners completed a survey about project contexts, processes, interventions, and outcomes. Factorial validity was established through confirmatory factor analysis supporting seven scales: contextual capacity, commitment to collective empowerment, relationships, community engagement in research actions, synergy, partner and partnership transformation, and projected outcomes. Convergent validity was established through examining covariances among the scales. This study largely yielded results consistent with a previous psychometric study of related measures, while demonstrating improved ceiling effects of the items and refined conceptualization of core theoretical constructs.},
  langid = {english},
  keywords = {Community-based participatory research,Community-engaged research,Empowerment,Measurement,Psychometrics},
  file = {/Users/jfprice/Zotero/storage/LMVEMPK9/Boursaw et al_2021_Scales of Practices and Outcomes for Community-Engaged Research.pdf;/Users/jfprice/Zotero/storage/BQWQUXIY/ajcp.html}
}

@report{oregondepartmentofenvironmentalqualityEnhancingCommunityEngagement2022,
  title = {Enhancing {{Community Engagement Using Q-Methodology}}},
  author = {Oregon Department of Environmental Quality},
  date = {2022},
  institution = {Oregon Department of Environmental Quality},
  location = {Portland, OR},
  url = {https://www.oregon.gov/deq/mm/Documents/recQmethodRep.pdf},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/Users/jfprice/Zotero/storage/AEEF4DDU/Oregon Department of Environmental Quality_2022_Enhancing Community Engagement Using Q-Methodology.pdf}
}

@article{morrisonExploringFacultyPerspectives2017,
  title = {Exploring {{Faculty Perspectives}} on {{Community Engaged Scholarship}}: {{The Case}} for {{Q Methodology}}},
  shorttitle = {Exploring {{Faculty Perspectives}} on {{Community Engaged Scholarship}}},
  author = {Morrison, Emily and Wagner, Wendy},
  date = {2017-02-22},
  journaltitle = {Michigan Journal of Community Service Learning},
  volume = {23},
  number = {1},
  issn = {1944-0219},
  doi = {10.3998/mjcsloa.3239521.0023.101},
  url = {http://hdl.handle.net/2027/spo.3239521.0023.101},
  urldate = {2023-11-01},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/UDKY74VS/Morrison_Wagner_2017_Exploring Faculty Perspectives on Community Engaged Scholarship.pdf}
}

@article{mukherjeeComparisonTechniquesEliciting2018,
  title = {Comparison of Techniques for Eliciting Views and Judgements in Decision-Making},
  author = {Mukherjee, Nibedita and Zabala, Aiora and Huge, Jean and Nyumba, Tobias Ochieng and Adem Esmail, Blal and Sutherland, William J.},
  date = {2018},
  journaltitle = {Methods in Ecology and Evolution},
  volume = {9},
  number = {1},
  pages = {54--63},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12940},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12940},
  urldate = {2023-10-20},
  abstract = {Decision-making is a complex process that typically includes a series of stages: identifying the issue, considering possible options, making judgements and then making a decision by combining information and values. The current status quo relies heavily on the informational aspect of decision-making with little or no emphasis on the value positions that affect decisions. There is increasing realization of the importance of adopting rigorous methods for each stage such that the information, views and judgements of stakeholders and experts are used in a systematic and repeatable manner. Though there are several methodological textbooks which discuss a plethora of social science techniques, it is hard to judge the suitability of any given technique for a given decision problem. In decision-making, the three critical aspects are “what” decision is to be made, “who” makes the decisions and “how” the decisions are made. The methods covered in this paper focus on “how” decisions can be made. We compare six techniques: Focus Group Discussion (FGD), Interviews, Q methodology, Multi-criteria Decision Analysis (MCDA), Nominal Group Technique and the Delphi technique specifically in the context of biodiversity conservation. All of these techniques (with the exception of MCDA) help in understanding human values and the underlying perspectives which shape decisions. Based on structured reviews of 423 papers covering all six methods, we compare the conceptual and logistical characteristics of the methods, and map their suitability for the different stages of the decision-making process. While interviews and FGD are well-known, techniques such the Nominal Group technique and Q methodology are relatively under-used. In situations where conflict is high, we recommend using the Q methodology and Delphi technique to elicit judgements. Where conflict is low, and a consensus is needed urgently, the Nominal Group technique may be more suitable. We present a nuanced synthesis of methods aimed at users. The comparison of the different techniques might be useful for project managers, academics or practitioners in the planning phases of their projects and help in making better informed methodological choices.},
  langid = {english},
  keywords = {conservation,decision-making,Delphi technique,focus group discussion,interview,multi-criteria decision,nominal group technique,Q methodology},
  file = {/Users/jfprice/Zotero/storage/7HFV2NC4/Mukherjee et al_2018_Comparison of techniques for eliciting views and judgements in decision-making.pdf;/Users/jfprice/Zotero/storage/NP32SHFR/2041-210X.html}
}

@article{beloneCommunityBasedParticipatoryResearch2016a,
  title = {Community-{{Based Participatory Research Conceptual Model}}: {{Community Partner Consultation}} and {{Face Validity}}},
  shorttitle = {Community-{{Based Participatory Research Conceptual Model}}},
  author = {Belone, Lorenda and family=Lucero, given=JE., given-i={{JE}} and Duran, B. and Tafoya, G. and family=Baker, given=EA., given-i={{EA}} and Chan, D. and Chang, C. and Greene-Moton, E. and Kelley, M. and Wallerstein, Nina},
  date = {2016-01},
  journaltitle = {Qualitative health research},
  shortjournal = {Qual Health Res},
  volume = {26},
  number = {1},
  eprint = {25361792},
  eprinttype = {pmid},
  pages = {117--135},
  issn = {1049-7323},
  doi = {10.1177/1049732314557084},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4839192/},
  urldate = {2023-10-18},
  abstract = {A national community based participatory research (CBPR) team developed a conceptual/logic model of CBPR partnerships to understand the contribution of partnership processes to improved community capacity and health outcomes. With the model primarily developed through academic literature and expert consensus-building, we sought community input to assess face validity and acceptability. Our research team conducted semi-structured focus groups with six partnerships nation-wide. Participants validated and expanded upon existing model constructs and identified new constructs based on “real-world” praxis, resulting in a revised model. Four cross-cutting constructs were identified: trust development, capacity, mutual learning, and power dynamics. By empirically testing the model, we found community face validity and capacity to adapt the model to diverse contexts. We recommend partnerships use and adapt the CBPR model and its constructs, for collective reflection and evaluation, to enhance their partnering practices and achieve their health and research goals.},
  pmcid = {PMC4839192},
  file = {/Users/jfprice/Zotero/storage/KNRED9LT/Belone et al_2016_Community-Based Participatory Research Conceptual Model.pdf}
}

@article{moreno-boteHeuristicsOptimalSolutions2020,
  title = {Heuristics and Optimal Solutions to the Breadth–Depth Dilemma},
  author = {Moreno-Bote, Rubén and Ramírez-Ruiz, Jorge and Drugowitsch, Jan and Hayden, Benjamin Y.},
  date = {2020-08-18},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {33},
  pages = {19799--19808},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2004929117},
  url = {https://www.pnas.org/doi/10.1073/pnas.2004929117},
  urldate = {2023-05-12},
  abstract = {In multialternative risky choice, we are often faced with the opportunity to allocate our limited information-gathering capacity between several options before receiving feedback. In such cases, we face a natural trade-off between breadth—spreading our capacity across many options—and depth—gaining more information about a smaller number of options. Despite its broad relevance to daily life, including in many naturalistic foraging situations, the optimal strategy in the breadth–depth trade-off has not been delineated. Here, we formalize the breadth–depth dilemma through a finite-sample capacity model. We find that, if capacity is small (∼10 samples), it is optimal to draw one sample per alternative, favoring breadth. However, for larger capacities, a sharp transition is observed, and it becomes best to deeply sample a very small fraction of alternatives, which roughly decreases with the square root of capacity. Thus, ignoring most options, even when capacity is large enough to shallowly sample all of them, is a signature of optimal behavior. Our results also provide a rich casuistic for metareasoning in multialternative decisions with bounded capacity using close-to-optimal heuristics.},
  file = {/Users/jfprice/Zotero/storage/NHTETDPD/Moreno-Bote et al_2020_Heuristics and optimal solutions to the breadth–depth dilemma.pdf}
}

@online{mcchesneyWhyYouShould2020,
  title = {Why You Should Summarize Your Data with the Geometric Mean},
  author = {McChesney, Jasper},
  date = {2020-10-16T13:30:52},
  url = {https://jlmc.medium.com/understanding-three-simple-statistics-for-data-visualizations-2619dbb3677a},
  urldate = {2024-05-28},
  abstract = {(or two other common statisitcs)},
  langid = {english},
  organization = {Medium},
  file = {/Users/jfprice/Zotero/storage/399FEPU7/understanding-three-simple-statistics-for-data-visualizations-2619dbb3677a.html}
}

@online{mcnicholAverageYouRe2022,
  title = {On {{Average}}, {{You}}’re {{Using}} the {{Wrong Average}}: {{Geometric}} \& {{Harmonic Means}} in {{Data Analysis}}},
  shorttitle = {On {{Average}}, {{You}}’re {{Using}} the {{Wrong Average}}},
  author = {McNichol, Daniel},
  date = {2022-01-24T01:18:13},
  url = {https://towardsdatascience.com/on-average-youre-using-the-wrong-average-geometric-harmonic-means-in-data-analysis-2a703e21ea0},
  urldate = {2024-05-28},
  abstract = {When the Mean Doesn’t Mean What You Think it Means},
  langid = {english},
  organization = {Medium},
  file = {/Users/jfprice/Zotero/storage/6B74IVRX/on-average-youre-using-the-wrong-average-geometric-harmonic-means-in-data-analysis-2a703e21ea0.html}
}

@article{smithSalienceCountsandDoes1997,
  title = {Salience Counts-and so Does Accuracy: {{Correcting}} and Updating a Measure for Free-List-Item Salience},
  shorttitle = {Salience Counts-and so Does Accuracy},
  author = {Smith, J. Jerome and Borgatti, Stephen P.},
  date = {1997},
  journaltitle = {Journal of Linguistic Anthropology},
  volume = {7},
  pages = {208--209},
  doi = {10.1525/jlin.1997.7.2.208},
  url = {http://qualquant.org/wp-content/uploads/cda/*smith%20and%20borgatti%201997%20salience%20and%20accuracy.pdf},
  urldate = {2017-08-23},
  file = {/Users/jfprice/Zotero/storage/3CSG5VAB/smith and borgatti 1997 salience and accuracy.pdf}
}

@software{rcoreteamLanguageEnvironmentStatistical2022,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {R Core Team},
  date = {2022},
  location = {Vienna, Austria},
  url = {https://www.R-project.org/},
  organization = {R Foundation for Statistical Computing}
}

@article{purzyckiAnthroToolsPackageCrossCultural2017a,
  title = {{{AnthroTools}}: {{An R Package}} for {{Cross-Cultural Ethnographic Data Analysis}}},
  shorttitle = {{{AnthroTools}}},
  author = {Purzycki, Benjamin Grant and Jamieson-Lane, Alastair},
  date = {2017-02},
  journaltitle = {Cross-Cultural Research},
  shortjournal = {Cross-Cultural Research},
  volume = {51},
  number = {1},
  pages = {51--74},
  issn = {1069-3971, 1552-3578},
  doi = {10.1177/1069397116680352},
  url = {http://journals.sagepub.com/doi/10.1177/1069397116680352},
  urldate = {2022-06-02},
  abstract = {As large-scale collaborative, cross-cultural ethnographic research becomes easier and easier to realize, certain ethnographic methods and analyses should be correspondingly more available, inviting, and accommodating. We have therefore created AnthroTools, a package for the free, opensource language R, with a variety of tools and functions suitable for both multi-factor free-list analysis and Bayesian cultural consensus modeling. Free-list data elicitation is a simple technique for ethnographic research. However, especially for cross-cultural free-list data, background preparation is considerable and often requires specific software. In addition, although current cultural consensus analysis tools offer very sophisticated analyses, they also either require specialized software or have computationally taxing methods. AnthroTools expedites these techniques, rapidly performs diagnostics, and prepares data for further analysis. In this article, we briefly discuss what this package offers cross-cultural researchers and provide basic examples of some of its functions.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/7XTX34G8/Purzycki_Jamieson-Lane_2017_AnthroTools.pdf}
}

@article{fiksUsingFreelistingUnderstand2011,
  title = {Using Freelisting to Understand Shared Decision Making in {{ADHD}}: {{Parents}}’ and Pediatricians’ Perspectives},
  shorttitle = {Using Freelisting to Understand Shared Decision Making in {{ADHD}}},
  author = {Fiks, Alexander G. and Gafen, Angela and Hughes, Cayce C. and Hunter, Kenya F. and Barg, Frances K.},
  date = {2011-08},
  journaltitle = {Patient education and counseling},
  shortjournal = {Patient Educ Couns},
  volume = {84},
  number = {2},
  eprint = {20797833},
  eprinttype = {pmid},
  pages = {236--244},
  issn = {0738-3991},
  doi = {10.1016/j.pec.2010.07.035},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3551534/},
  urldate = {2024-07-25},
  abstract = {Objective To compare and contrast notions of ADHD among pediatricians and parents of affected children to understand the perspectives they bring to shared decision making (SDM). Methods In this freelisting study, 60 parents of children with ADHD and 30 primary care pediatricians listed words reflecting their understanding of (1) Attention Deficit Hyperactivity Disorder (ADHD), (2) getting/offering help for ADHD, (3) talking to doctors/families about ADHD, and (4) “mental health.” Smith’s salience score established terms that were salient and cultural consensus analysis identified variation within subgroups of participants. Results Parents’ terms reflected ADHD’s effects on the child and family, while clinicians often mentioned school. Lists suggested differing needs and goals for clinicians and subgroups of parents in SDM: “time” for clinicians, “learning” and “understanding” for non-college educated parents, and “comfort” and “relief” for college educated parents. Neither parents nor clinicians framed ADHD in the same way as “mental health.” Conclusion Parents and clinicians, who conceptualize ADHD differently, should negotiate a shared understanding of ADHD as a basis for SDM. Treatment discussions should be tailored to encompass families’ varied emotional and educational needs. Practice implications Fostering SDM in primary care is consonant with notions of ADHD as distinct from mental health.},
  pmcid = {PMC3551534},
  file = {/Users/jfprice/Zotero/storage/L747QLRJ/Fiks et al_2011_Using freelisting to understand shared decision making in ADHD.pdf}
}

@incollection{quinlanFreelistingMethod2018,
  title = {The {{Freelisting Method}}},
  booktitle = {Handbook of {{Research Methods}} in {{Health Social Sciences}}},
  author = {Quinlan, Marsha B.},
  editor = {Liamputtong, Pranee},
  date = {2018},
  pages = {1--16},
  publisher = {Springer Singapore},
  location = {Singapore},
  doi = {10.1007/978-981-10-2779-6_12-2},
  url = {http://link.springer.com/10.1007/978-981-10-2779-6_12-2},
  urldate = {2022-08-19},
  abstract = {A freelist is a mental inventory of items an individual thinks of within a given category. Freelists reveal cultural “salience” of particular notions within groups, and variation in individuals’ topical knowledge across groups. The ease and accuracy of freelist interviewing, or freelisting, makes it ideal for collecting data on health knowledge and beliefs from relatively large samples. Successful freelisting requires researchers to break the research topic into honed categories. Research participants presented with broad prompts tend to “unpack” mental subcategories and may omit (forget) common items or categories. Researchers should find subdomains to present individually for participants to unpack in separate smaller freelists. Researchers may focus the freelist prompts through successive freelisting, pile sorts, or focus group-interviews. Written freelisting among literate populations allows for rapid data collection, possibly from multiple individuals simultaneously. Among nonliterate peoples, using oral freelists remains a relatively rapid method; however, interviewers must prevent bystanders from “contaminating” individual interviewees’ lists. Researchers should cross-check freelist responses with informal methods as much as practicable to contextualize and understand the references therein. With proper attention to detail, freelisting can amass high quality data on people’s medical understanding, attitudes, and behaviors.},
  isbn = {978-981-10-2779-6},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/TXJP43EH/Quinlan_2018_The Freelisting Method.pdf}
}

@book{christakisConnectedSurprisingPower2009,
  title = {Connected: {{The}} Surprising Power of Our Social Networks and How They Shape Our Lives},
  author = {Christakis, Nicholas A. and Fowler, James H.},
  date = {2009},
  publisher = {{Little, Brown and Company}},
  location = {New York}
}

@online{HowMakeSpirals,
  title = {How to Make Spirals with {{R}} and Make Indigenous Communities Count},
  author = {Datasketch},
  date = {2023},
  url = {https://datasketch.dev/post/2020-08-09-how-to-make-spirals-with-r/},
  urldate = {2024-08-25},
  file = {/Users/jfprice/Zotero/storage/WUT6RRJG/2020-08-09-how-to-make-spirals-with-r.html}
}

@article{reebPsychoEcologicalSystemsModel2017,
  title = {Psycho-{{Ecological Systems Model}}: {{A Systems Approach}} to {{Planning}} and {{Gauging}} the {{Community Impact}} of {{Community-Engaged Scholarship}}},
  shorttitle = {Psycho-{{Ecological Systems Model}}},
  author = {Reeb, Roger and Snow-Hill, Nyssa and Folger, Susan and Steel, Anne and Stayton, Laura and Hunt, Charles and O’Koon, Bernadette and Glendening, Zachary},
  date = {2017-10-01},
  journaltitle = {Michigan Journal of Community Service Learning},
  volume = {24},
  number = {1},
  doi = {10.3998/mjcsloa.3239521.0024.102},
  url = {https://ecommons.udayton.edu/psy_fac_pub/28},
  file = {/Users/jfprice/Zotero/storage/HCVXNCTQ/Reeb et al_2017_Psycho-Ecological Systems Model.pdf}
}

@article{aroraMeasuringCommunityBasedParticipatory2015,
  title = {Measuring {{Community-Based Participatory Research Partnerships}}: {{The Initial Development}} of an {{Assessment Instrument}}},
  shorttitle = {Measuring {{Community-Based Participatory Research Partnerships}}},
  author = {Arora, Prerna G. and Krumholz, Lauren S. and Guerra, Terry and Leff, Stephen S.},
  date = {2015},
  journaltitle = {Progress in Community Health Partnerships: Research, Education, and Action},
  volume = {9},
  number = {4},
  pages = {549--560},
  publisher = {Johns Hopkins University Press},
  issn = {1557-055X},
  doi = {10.1353/cpr.2015.0077},
  url = {https://muse.jhu.edu/pub/1/article/602939},
  urldate = {2024-03-17},
  abstract = {Background: Although the partnership between academic researchers and community members is paramount to community-based research efforts, a limited number of measures exist to evaluate this construct. Of those in existence, no assessment measures include a comprehensive coverage of the many dimensions of partnerships. In addition, these measures were not designed through an extensive community-based participatory research (CBPR) model, in which the strengths of traditional assessment techniques were integrated with input from stakeholders., Objectives: The purpose of this article was to describe the creation of a measure to evaluate key dimensions of partnerships forged between researchers and community members using a CBPR approach to measurement development., Methods: The iterative process of developing this measure consisted of integrating valuable feedback from community partners and researchers, via multiple rounds of item sorting and qualitative interviewing., Results: The resultant measure, titled Partnership Assessment In community-based Research (PAIR), consists of 32 items, and comprises 5 dimensions: communication, collaboration, partnership values, benefits, and evaluation. The innovative process of using CBPR in the development of measures, the benefits of this approach, and the lessons learned are highlighted., Conclusions: PAIR was developed out of a need identified jointly by community members and researchers, and is intended to characterize the range of relationships between researchers and community members engaging in community-based research and programming.},
  keywords = {Community-based participatory research,community-institutional relations,methods,qualitative research,questionnaires},
  file = {/Users/jfprice/Zotero/storage/49RQLB3L/Arora et al_2015_Measuring Community-Based Participatory Research Partnerships.pdf}
}

@article{muhlesteinAssessingCommunityEngagedLearning2019,
  title = {Assessing {{Community-Engaged Learning Impacts}} Using {{Ripple Effects Mapping}}},
  author = {Muhlestein, Benjamin and McCann, Roslynn},
  date = {2019-12-19},
  journaltitle = {Journal on Empowering Teaching Excellence},
  volume = {3},
  number = {2},
  issn = {2644-2132},
  doi = {10.15142/zwmv-4273},
  url = {https://digitalcommons.usu.edu/jete/vol3/iss2/5},
  file = {/Users/jfprice/Zotero/storage/DM66VZCU/5.html}
}

@article{zimmermanAssessingImpactsRipple2019,
  title = {Assessing the {{Impacts}} and {{Ripple Effects}} of a {{Community}}–{{University Partnership}}: {{A Retrospective Roadmap}}},
  shorttitle = {Assessing the {{Impacts}} and {{Ripple Effects}} of a {{Community}}–{{University Partnership}}},
  author = {Zimmerman, Emily B. and Creighton, Gwen Corley and Miles, Chimere and Cook, Sarah and Haley, Amber and Bea, Chanel and Robles, Andrea and Aroche, Alicia},
  date = {2019},
  journaltitle = {Michigan journal of community service learning},
  shortjournal = {Mich J Community Serv Learn},
  volume = {25},
  number = {1},
  eprint = {32905315},
  eprinttype = {pmid},
  pages = {62--76},
  issn = {1076-0180},
  doi = {10.3998/mjcsloa.3239521.0025.106},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7470035/},
  urldate = {2024-03-17},
  abstract = {Each community-based participatory research (CBPR) partnership may incur “ripple effects” – impacts that happen outside the scope of planned projects. We used brainstorming and interviewing to create a roadmap that incorporated input from nine CBPR participants and five community/academic partners to retrospectively assess the ripple effects observed after five years of participatory research in one urban community. The resulting roadmap reflected a range of community impacts which we then divided into four key areas: impacts in the community (i.e., strategies, programs, and policies implemented by community partners), impacts on the CBPR team, impacts on individuals (participants and community members), and contributions to the field and the university. Our approach focused on observing what happened in the community that was directly or indirectly related to our partnership, process, products, and relationships. Much of the impact we observed reflected the synergy of sharing our research and community voice with responsive partners and stakeholders.},
  pmcid = {PMC7470035},
  file = {/Users/jfprice/Zotero/storage/RE8HEIC3/Zimmerman et al_2019_Assessing the Impacts and Ripple Effects of a Community–University Partnership.pdf}
}

@book{chazdonFieldGuideRipple2017,
  title = {A {{Field Guide}} to {{Ripple Effects Mapping}}},
  author = {Chazdon, Scott and Emery, Mary and Hansen, Debra and Higgins, Lorie and Sero, Rebecca},
  date = {2017-10},
  publisher = {University of Minnesota Libraries Publishing},
  url = {http://conservancy.umn.edu/handle/11299/190639},
  urldate = {2022-10-22},
  abstract = {The second volume in the Minnesota Evaluation Studies Institute Program Evaluation Series focuses on the emerging evaluation technique of Ripple Effects Mapping (REM). This participatory data collection method is designed to capture the impact of complex programs and collaborative processes. Well-suited for evaluating group-focused efforts, Ripple Effects Mapping involves aspects of Appreciative Inquiry, mind mapping, facilitated discussion, and qualitative data analysis. As the REM process unfolds, the intended and unintended impacts of participant efforts are visually displayed in a way that encourages discussion and engagement. Using these visuals, plus other graphics, pictures, and real-life examples of how Ripple Effects Mapping has been successfully used in multiple settings, this book provides a comprehensive overview of REM. Providing an in-depth examination of the origins, elements, and how-to of the REM process, the Field Guide to Ripple Effects Mapping is a step by-step guide to successfully implementing this process with a group, collaboration, or community of choice.},
  langid = {english},
  annotation = {Accepted: 2017-10-10T19:56:23Z},
  file = {/Users/jfprice/Zotero/storage/6WV9QPII/Chazdon et al_2017_A Field Guide to Ripple Effects Mapping.pdf;/Users/jfprice/Zotero/storage/UUB7VZ74/190639.html}
}

@article{claytonDifferentiatingAssessingRelationships2010,
  title = {Differentiating and {{Assessing Relationships}} in {{Service-Learning}} and {{Civic Engagement}}: {{Exploitative}}, {{Transactional}}, or {{Transformational}}},
  author = {Clayton, Patti H and Bringle, Robert G and Senor, Bryanne and Huq, Jenny and Morrison, Mary},
  date = {2010},
  journaltitle = {Michigan Journal of Community Service Learning},
  pages = {5--22},
  url = {https://indiana.sharepoint.com/sites/msteams_92d57a/Shared%20Documents/Forms/AllItems.aspx?id=%2Fsites%2Fmsteams%5F92d57a%2FShared%20Documents%2FGeneral%2Fcsl%2FWCMS%20Resources%2Ftres1%2Epdf&parent=%2Fsites%2Fmsteams%5F92d57a%2FShared%20Documents%2FGeneral%2Fcsl%2FWCMS%20Resources&p=true&ga=1},
  urldate = {2024-08-26},
  keywords = {⛔ No DOI found},
  file = {/Users/jfprice/Zotero/storage/DU8I3E8V/AllItems.html}
}

@article{kniffinRelationshipsPartnershipsCommunity2020,
  title = {Relationships and {{Partnerships}} in {{Community}}–{{Campus Engagement}}: {{Evolving Inquiry}} and {{Practice}}},
  shorttitle = {Relationships and {{Partnerships}} in {{Community}}–{{Campus Engagement}}},
  author = {Kniffin, Lori and Camo-Biogradlija, Jasmina and Price, Mary F. and Kohl, Emily and Dickovick, Alessandra Del Conte and Williams, Jamie and Goodwin, Jamie and Johnson, Kristi V. and Clayton, Patti H. and Bringle, Robert G.},
  date = {2020-12-30},
  journaltitle = {International Journal of Research on Service-Learning and Community Engagement},
  volume = {8},
  number = {1},
  issn = {2374-9466},
  doi = {10.37333/001c.18586},
  url = {https://ijrslce.scholasticahq.com/article/18586-relationships-and-partnerships-in-community-campus-engagement-evolving-inquiry-and-practice},
  urldate = {2024-08-26},
  abstract = {Inquiry and practice related to community–campus partnerships are ever evolving, with significant current momentum toward democratic engagement. To inform the ongoing development of associated practitioner scholarship, we examine the development of a tool for assessing the quality of community–campus relationships, the Transformational Relationship Evaluation Scale (TRES), as a microcosm of some underlying dynamics in previous and current work. After an overview of its conceptual foundations, we present TRES, review examples of its uses across multiple contexts, and share lessons learned from critical reflection on those uses along with associated implications for the future development of such tools. Subsequent discussion focuses on shifts toward conceptualizing both partnerships themselves and processes of inquiring into them in terms of systems and co-creation. Seeking to support readers in operationalizing democratic engagement in their inquiry and practice, we share conceptual frameworks, tangible tools, guiding questions for research, and reflective critique on our experience as practitioner-scholars of partnerships.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/BYI6TEHA/Kniffin et al_2020_Relationships and Partnerships in Community–Campus Engagement.pdf}
}

@article{schulzInstrumentEvaluatingDimensions2003,
  title = {Instrument for Evaluating Dimensions of Group Dynamics within Community-Based Participatory Research Partnerships},
  author = {Schulz, Amy J. and Israel, Barbara A. and Lantz, Paula},
  date = {2003},
  journaltitle = {Evaluation and Program Planning},
  volume = {26},
  number = {3},
  pages = {249--262},
  publisher = {Elsevier Science},
  location = {Netherlands},
  issn = {1873-7870},
  doi = {10.1016/S0149-7189(03)00029-6},
  abstract = {We describe the development, adaptation, and use of evaluation approaches assessing key dimensions of group partnerships. A review of relevant literature describes the rationale for the evaluation of partnership dynamics, and the selection of relevant dimensions for evaluation and assessment. Three case studies are presented to illustrate the use of this evaluation instrument in community-based participatory research partnerships to assess key dimensions of partnership process. The use of evaluation results in self-assessment and partnership development are described and lessons learned in the application of these results are discussed. Finally, we discuss the potential, challenges, and areas for further development of evaluation tools to assess group dynamics in partnership efforts. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Community Development,Experimentation,Group Dynamics,Group Participation},
  file = {/Users/jfprice/Zotero/storage/JSZIN6HD/Schulz et al_2003_Instrument for evaluating dimensions of group dynamics within community-based.pdf;/Users/jfprice/Zotero/storage/EXINUKW7/2003-99854-002.html}
}

@article{keyContinuumCommunityEngagement2019,
  title = {The {{Continuum}} of {{Community Engagement}} in {{Research}}: {{A Roadmap}} for {{Understanding}} and {{Assessing Progress}}},
  shorttitle = {The {{Continuum}} of {{Community Engagement}} in {{Research}}},
  author = {Key, Kent D. and Furr-Holden, Debra and Lewis, E. Yvonne and Cunningham, Rebecca and Zimmerman, Marc A. and Johnson-Lawrence, Vicki and Selig, Suzanne},
  date = {2019},
  journaltitle = {Progress in Community Health Partnerships: Research, Education, and Action},
  shortjournal = {Progress in Community Health Partnerships},
  volume = {13},
  number = {4},
  pages = {427--434},
  issn = {1557-055X},
  doi = {10.1353/cpr.2019.0064},
  url = {https://muse.jhu.edu/article/743598},
  urldate = {2024-03-07},
  abstract = {Background: The past two decades have been marked by increased community involvement in the research process. Community-engaged research (CEnR) is increasingly promoted in the literature, and academic programs with a community–academic partnership focus. Community-based participatory research (CBPR) is an approach to frame equitable community involvement in research and is a critical component of the CEnR continuum. As with CEnR, noted benefits of using CBPR expressed in the literature, which include enhancing the relevance and application of the research data, expertise to complex problems at all stages of research, overcoming community distrust, and improving community health. This article presents a community engagement (CE) model that includes seven defined designations for CEnR. In addition, this model includes equity indicators and contextual factors for consideration at the various levels of engagement along the continuum.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/YL5PYBER/Key et al_2019_The Continuum of Community Engagement in Research.pdf}
}

@article{lugerMeasuringCommunityEngagedResearch2020,
  title = {Measuring {{Community-Engaged Research Contexts}}, {{Processes}}, and {{Outcomes}}: {{A Mapping Review}}},
  shorttitle = {Measuring {{Community-Engaged Research Contexts}}, {{Processes}}, and {{Outcomes}}},
  author = {Luger, Tana M. and Hamilton, Alison B. and True, Gala},
  date = {2020},
  journaltitle = {The Milbank Quarterly},
  volume = {98},
  number = {2},
  pages = {493--553},
  issn = {1468-0009},
  doi = {10.1111/1468-0009.12458},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0009.12458},
  urldate = {2024-03-16},
  abstract = {Policy Points Community-engaged research (CEnR) engenders meaningful academic-community partnerships to improve research quality and health outcomes. CEnR has increasingly been adopted by health care systems, funders, and communities looking for solutions to intractable problems. It has been difficult to systematically measure CEnR's impact, as most evaluations focus on project-specific outcomes. Similarly, partners have struggled with identifying appropriate measures to assess outcomes of interest. To make a case for CEnR's value, we must demonstrate the impacts of CEnR over time. We compiled recent measures and developed an interactive data visualization to facilitate more consistent measurement of CEnR's theoretical domains. Context Community-engaged research (CEnR) aims to engender meaningful academic-community partnerships to increase research quality and impact, improve individual and community health, and build capacity for uptake of evidence-based practices. Given the urgency to solve society's pressing public health problems and increasing competition for funding, it is important to demonstrate CEnR's value. Most evaluations focus on project-specific outcomes, making it difficult to demonstrate CEnR's broader impact. Moreover, it is challenging for partnerships to identify assessments of interest beyond process measures. We conducted a mapping review to help partnerships find and select measures to evaluate CEnR projects and to characterize areas where further development of measures is needed. Methods We searched electronic bibliographic databases using relevant search terms from 2009 to 2018 and scanned CEnR projects to identify unpublished measures. Through review and reduction, we found 69 measures of CEnR's context, process, or outcomes that are potentially generalizable beyond a specific health condition or population. We abstracted data from descriptions of each measure to catalog purpose, aim (context, process, or outcome), and specific domains being measured. Findings We identified 28 measures of the conditions under which CEnR is conducted and factors to support effective academic-community collaboration (context); 43 measures evaluating constructs such as group dynamics and trust (process); and 43 measures of impacts such as benefits and challenges of CEnR participation and system and capacity changes (outcomes). Conclusions We found substantial variation in how academic-community partnerships conceptualize and define even similar domains. Achieving more consistency in how partnerships evaluate key constructs could reduce measurement confusion apparent in the literature. A hybrid approach whereby partnerships discuss common metrics and develop locally important measures can address CEnR's multiple goals. Our accessible data visualization serves as a convenient resource to support partnerships’ evaluation goals and may help to build the evidence base for CEnR through the use of common measures across studies.},
  langid = {english},
  keywords = {action research,community-engaged research,mapping review,measurement,outcomes},
  file = {/Users/jfprice/Zotero/storage/2IBWSY2E/Luger et al_2020_Measuring Community-Engaged Research Contexts, Processes, and Outcomes.pdf;/Users/jfprice/Zotero/storage/PSE8R7YK/1468-0009.html}
}

@article{bowenSystematicReviewQuantitative2017,
  title = {Systematic {{Review}} of {{Quantitative Measures}} of {{Stakeholder Engagement}}},
  author = {Bowen, D. J. and Hyams, T. and Goodman, M. and West, K. M. and Harris-Wai, J. and Yu, J.-H.},
  date = {2017-09},
  journaltitle = {Clinical and Translational Science},
  shortjournal = {Clin Transl Sci},
  volume = {10},
  number = {5},
  eprint = {28556620},
  eprinttype = {pmid},
  pages = {314--336},
  issn = {1752-8062},
  doi = {10.1111/cts.12474},
  langid = {english},
  pmcid = {PMC5593160},
  keywords = {Humans,Publications,Review Literature as Topic,Stakeholder Participation},
  file = {/Users/jfprice/Zotero/storage/CN6IRZC6/Bowen et al_2017_Systematic Review of Quantitative Measures of Stakeholder Engagement.pdf}
}

@article{goodmanEVALUATINGCOMMUNITYENGAGEMENT2017,
  title = {{{EVALUATING COMMUNITY ENGAGEMENT IN RESEARCH}}: {{QUANTITATIVE MEASURE DEVELOPMENT}}},
  shorttitle = {{{EVALUATING COMMUNITY ENGAGEMENT IN RESEARCH}}},
  author = {Goodman, Melody S. and Sanders Thompson, Vetta L. and Johnson, Cassandra Arroyo and Gennarelli, Renee and Drake, Bettina F. and Bajwa, Pravleen and Witherspoon, Maranda and Bowen, Deborah},
  date = {2017-01},
  journaltitle = {Journal of community psychology},
  shortjournal = {J Community Psychol},
  volume = {45},
  number = {1},
  eprint = {29302128},
  eprinttype = {pmid},
  pages = {17--32},
  issn = {0090-4392},
  doi = {10.1002/jcop.21828},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5749252/},
  urldate = {2024-08-26},
  abstract = {Although the importance of community engagement in research has been previously established, there are few evidence-based approaches for measuring the level of community engagement in research projects. A quantitative community engagement measure was developed, aligned with 11 engagement principles (EPs) previously established in the literature. The measure has 96 Likert response items; 3–5 quality items and 3–5 quantity items measure each EP. Cronbach’s alpha is used to examine the internal consistency of items that measure a single EP. Every EP item group had a Cronbach’s alpha {$>$} .85, which indicates strong internal consistency for all question groups across both scales (quality and quantity). This information determines the level of community engagement, which can be correlated with other research outcomes.},
  pmcid = {PMC5749252},
  file = {/Users/jfprice/Zotero/storage/YP8APBAP/Goodman et al_2017_EVALUATING COMMUNITY ENGAGEMENT IN RESEARCH.pdf}
}

@article{alvarezParticipatoryImpactPathways2010,
  title = {Participatory {{Impact Pathways Analysis}}: A Practical Method for Project Planning and Evaluation},
  shorttitle = {Participatory {{Impact Pathways Analysis}}},
  author = {Alvarez, Sophie and Douthwaite, Boru and Thiele, Graham and Mackay, Ronald and Cordoba, Diana and Tehelen, Katherine},
  date = {2010-11-01},
  journaltitle = {Development in Practice},
  shortjournal = {Development in Practice},
  volume = {20},
  pages = {946--958},
  doi = {10.2307/20787374},
  abstract = {Participatory Impact Pathways Analysis (PIPA) is a practical approach to planning, monitoring and evaluation, developed for use with complex research-for-development projects. PIPA begins with a participatory workshop where stakeholders make explicit their assumptions about how their project will make an impact, and produce an ‘Outcomes logic model’ and an ‘Impact logic model’. These two logic models provide an ex-ante framework of predictions of impact that can also be used in priority setting and ex-post impact assessment. PIPA engages stakeholders in a structured participatory process, promoting learning and providing a framework for ‘action research’ on processes of change.},
  keywords = {⚠️ Invalid DOI}
}

@article{shephardValuingEvaluatingCommunityengaged2018,
  title = {Valuing and Evaluating Community-Engaged Scholarship},
  author = {Shephard, Kerry and Brown, Kim and Guiney, Tess and Deaker, Lynley},
  date = {2018-03-01},
  journaltitle = {Tertiary Education and Management},
  shortjournal = {Tert Educ Manag},
  volume = {24},
  number = {1},
  pages = {83--94},
  issn = {1573-1936},
  doi = {10.1080/13583883.2017.1395904},
  url = {https://doi.org/10.1080/13583883.2017.1395904},
  urldate = {2024-08-26},
  abstract = {This article examines the nature of, and need for, evaluation of community-engaged university teaching and research. The research was conducted as part of a larger project aimed at improving institutional understanding of how to best support community-engaged university people. We interviewed 25 community-engaged colleagues, and used a general inductive approach to identify four recurring themes relating to evaluation within interview transcripts. The themes emphasised diverse conceptualisations of the nature of evaluation in this context, and concomitant concerns about where their community engagement fits within our higher education institution. Our research may help our institution, and others, to decide how best to provide institutional support to university people who choose to become community-engaged.},
  langid = {english},
  keywords = {academic roles,community engagement,educational evaluation,functions for higher education,scholarship of engagement,student placement}
}

@article{dostilioDemocraticallyEngagedCommunity2014,
  title = {Democratically {{Engaged Community}}–{{University Partnerships}}: {{Reciprocal Determinants}} of {{Democratically Oriented Roles}} and {{Processes}}},
  shorttitle = {Democratically {{Engaged Community}}–{{University Partnerships}}},
  author = {Dostilio, Lino D.},
  date = {2014-12-24},
  journaltitle = {Journal of Higher Education Outreach and Engagement},
  volume = {18},
  number = {4},
  pages = {235--244},
  issn = {2164-8212},
  url = {https://openjournals.libs.uga.edu/jheoe/article/view/1159},
  urldate = {2024-08-26},
  abstract = {A relatively new conception of engagement provides a framework by which institutions of higher education engage with communities in democratic ways, which include inclusive, reciprocal problem-oriented work that brings together university and community stakeholders as co-generators of knowledge. The resulting democratically engaged partnerships position diverse members to take on roles as collaborators and problem solvers. They are mutually transformed through the processes of reciprocation, power diffusion, and knowledge generation. How these roles and processes emerge is unknown. Neither the literature on democratic engagement nor that on community–university partnerships address this gap. Using a purposefully selected community–university partnership that has a high degree of democratic engagement, evidence was collected of the ways in which the roles and processes of democratically engaged partnerships were enacted. Of particular interest were the blend of democratic and technocratic characteristics present, the critical role of orienting new partners, and the role of leadership in promoting a democratic orientation.},
  issue = {4},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/Users/jfprice/Zotero/storage/V98PJKX3/Dostilio_2014_Democratically Engaged Community–University Partnerships.pdf}
}

@article{longBridgesBrokersBoundary2013,
  title = {Bridges, Brokers and Boundary Spanners in Collaborative Networks: {{A}} Systematic Review},
  shorttitle = {Bridges, Brokers and Boundary Spanners in Collaborative Networks},
  author = {Long, Janet and Cunningham, Frances and Braithwaite, Jeffrey},
  date = {2013-04-30},
  journaltitle = {BMC health services research},
  shortjournal = {BMC health services research},
  volume = {13},
  pages = {158},
  doi = {10.1186/1472-6963-13-158},
  abstract = {Background:  Bridges, brokers and boundary spanners facilitate transactions and the flow of information between people or groups who either have no physical or cognitive access to one another, or alternatively, who have no basis on which to trust each other. The health care sector is a context that is rich in isolated clusters, such as silos and professional "tribes," in need of connectivity. It is a key challenge in health service management to understand, analyse and exploit the role of key agents who have the capacity to connect disparate groupings in larger systems. Methods: The empirical, peer reviewed, network theory literature on brokerage roles was reviewed for the years 1994 to 2011 following PRISMA guidelines. Results: The 24 articles that made up the final literature set were from a wide range of settings and contexts not just healthcare. Methods of data collection, analysis, and the ways in which brokers were identified varied greatly. We found four main themes addressed in the literature: identifying brokers and brokerage opportunities, generation and integration of innovation, knowledge brokerage, and trust. The benefits as well as the costs of brokerage roles were examined. Conclusions: Collaborative networks by definition, seek to bring disparate groups together so that they can work effectively and synergistically together. Brokers can support the controlled transfer of specialised knowledge between groups, increase cooperation by liaising with people from both sides of the gap, and improve efficiency by introducing "good ideas" from one isolated setting into another.There are significant costs to brokerage. Densely linked networks are more efficient at diffusing information to all their members when compared to sparsely linked groups. This means that while a bridge across a structural hole allows information to reach actors that were previously isolated, it is not the most efficient way to transfer information. Brokers who become the holders of, or the gatekeepers to, specialised knowledge or resources can become overwhelmed by the role and so need support in order to function optimally.},
  file = {/Users/jfprice/Zotero/storage/XH5ENZN2/Long et al_2013_Bridges, brokers and boundary spanners in collaborative networks.pdf}
}

@article{wendlingEvaluatingCommunityEngagedResearch2023,
  title = {Evaluating {{Community-Engaged Research}} in {{Promotion}} and {{Tenure}}},
  author = {Wendling, Lauren Allen},
  date = {2023-09-18},
  journaltitle = {Metropolitan Universities},
  shortjournal = {MUJ},
  volume = {34},
  number = {5},
  issn = {1047-8485},
  doi = {10.18060/26658},
  url = {https://journals.iupui.edu/index.php/muj/article/view/26658},
  urldate = {2023-10-29},
  abstract = {To advance and encourage partnerships between institutions and their greater communities, academic reward structures must be designed in ways that support those who choose to leverage their expertise, resources, and time to engage with community in meaningful and mutually beneficial ways. This study investigates how school- and department-level promotion and tenure committees define, understand, and evaluate faculty’s engaged research. Specifically, this study explored what goes into making evaluative decisions and how evaluative decisions are made (e.g., how review committees define and categorize faculty’s engaged research, what metrics are used to assess it). In this single case multi-site qualitative study 12 participants across five R1 institutions classified as engaged by the Carnegie Foundation participated in semi-structured interviews. All participants were tenured, engaged scholars with experience serving on a school- and/or department-level promotion and tenure review committee. Findings demonstrate that review committees struggle to define, categorize, and evaluate community engaged research in promotion and tenure, as they are forced to exclusively rely on a traditional set of metrics to evaluate the engaged work of their peers. Though universities are making strides to institutionalize engagement, appropriate recognition of engaged research within promotion and tenure is not yet a reality.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/Y9NYLB6B/Wendling_2023_Evaluating Community-Engaged Research in Promotion and Tenure.pdf}
}

@article{reedProgramEvaluationCommunityengaged2015,
  title = {Program Evaluation as Community-Engaged Research: {{Challenges}} and Solutions},
  shorttitle = {Program Evaluation as Community-Engaged Research},
  author = {Reed, Richard},
  date = {2015},
  journaltitle = {Gateways: International Journal of Community Research and Engagement},
  volume = {8},
  number = {1},
  pages = {118--138},
  doi = {10.5130/ijcre.v8i1.4105},
  file = {/Users/jfprice/Zotero/storage/S8P3T9WZ/Reed_2015_Program evaluation as community-engaged research.pdf;/Users/jfprice/Zotero/storage/D2E7NNCT/informit.html}
}

@article{lewisLongitudinalMultisiteEvaluation2023,
  title = {A Longitudinal Multi-Site Evaluation of Community-Based Partnerships: Implications for Researchers, Funders, and Communities},
  shorttitle = {A Longitudinal Multi-Site Evaluation of Community-Based Partnerships},
  author = {Lewis, Virginia J. and Scott, Catherine M. and Silburn, Kate and Miller, William L.},
  date = {2023-10-03},
  journaltitle = {Health Research Policy and Systems},
  shortjournal = {Health Res Policy Sys},
  volume = {21},
  number = {1},
  pages = {103},
  issn = {1478-4505},
  doi = {10.1186/s12961-023-01045-y},
  url = {https://doi.org/10.1186/s12961-023-01045-y},
  urldate = {2024-04-02},
  abstract = {Innovative Models Promoting Access to Care Transformation (IMPACT) was a five-year (2013–2018), Canadian-Australian research program that aimed to use a community-based partnership approach to transform primary health care (PHC)~organizational structures to improve access to appropriate care for vulnerable populations. Local Innovation Partnerships (LIPs) were developed to support the IMPACT research program, and to be ongoing structures that would continue to drive local improvements to PHC.},
  langid = {english},
  keywords = {Community academic partnerships,Community-based participatory research,Implementing innovative interventions,Longitudinal qualitative evaluation,Multi-sectorial research partnerships},
  file = {/Users/jfprice/Zotero/storage/HHPXDZAH/Lewis et al_2023_A longitudinal multi-site evaluation of community-based partnerships.pdf}
}

@article{hauserMetricsYouAre1998,
  title = {Metrics: You Are What You Measure!},
  shorttitle = {Metrics},
  author = {Hauser, John and Katz, Gerald},
  date = {1998-10},
  journaltitle = {European Management Journal},
  shortjournal = {European Management Journal},
  volume = {16},
  number = {5},
  pages = {517--528},
  issn = {02632373},
  doi = {10.1016/S0263-2373(98)00029-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0263237398000292},
  urldate = {2024-05-31},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/CTGYBTLE/Hauser_Katz_1998_Metrics.pdf}
}

@inproceedings{kunduNewCentralityMeasure2011,
  title = {A {{New Centrality Measure}} for {{Influence Maximization}} in {{Social Networks}}},
  booktitle = {Pattern {{Recognition}} and {{Machine Intelligence}}},
  author = {Kundu, Suman and Murthy, C. A. and Pal, S. K.},
  editor = {Kuznetsov, Sergei O. and Mandal, Deba P. and Kundu, Malay K. and Pal, Sankar K.},
  date = {2011},
  pages = {242--247},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-21786-9_40},
  abstract = {The paper addresses the problem of finding top k influential nodes in large scale directed social networks. We propose a centrality measure for independent cascade model, which is based on diffusion probability (or propagation probability) and degree centrality. We use (i) centrality based heuristics with the proposed centrality measure to get k influential individuals. We have also found the same using (ii) high degree heuristics and (iii) degree discount heuristics. A Monte-Carlo simulation has been conducted with top k-nodes found through different methods. The result of simulation indicates, k nodes obtained through (i) significantly outperform those obtain by (ii) and (iii). We further verify the differences statistically using T-Test and found the minimum significance level (p-value) when k\,{$>$}\,5 is 0.022 compare with (ii) and 0.015 when comparing with (iii) for twitter data.},
  isbn = {978-3-642-21786-9},
  langid = {english},
  keywords = {Centrality Measure,Heuristic Model,High Degree Node,Maximization Problem,Social Network},
  file = {/Users/jfprice/Zotero/storage/QYZLV7FU/Kundu et al_2011_A New Centrality Measure for Influence Maximization in Social Networks.pdf}
}

@article{brinAnatomyLargescaleHypertextual1998,
  title = {The Anatomy of a Large-Scale Hypertextual {{Web}} Search Engine},
  author = {Brin, Sergey and Page, Lawrence},
  date = {1998-04},
  journaltitle = {Computer Networks and ISDN Systems},
  shortjournal = {Computer Networks and ISDN Systems},
  volume = {30},
  number = {1-7},
  pages = {107--117},
  issn = {01697552},
  doi = {10.1016/S0169-7552(98)00110-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S016975529800110X},
  urldate = {2024-08-29},
  abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/3EL6BYWY/Brin_Page_1998_The anatomy of a large-scale hypertextual Web search engine.pdf}
}

@article{luLeadersSocialNetworks2011,
  title = {Leaders in {{Social Networks}}, the {{Delicious Case}}},
  author = {Lü, Linyuan and Zhang, Yi-Cheng and Yeung, Chi Ho and Zhou, Tao},
  editor = {Scalas, Enrico},
  date = {2011-06-27},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS ONE},
  volume = {6},
  number = {6},
  pages = {e21202},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0021202},
  url = {https://dx.plos.org/10.1371/journal.pone.0021202},
  urldate = {2023-04-29},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/FJWYBDQM/Lu et al_2011_Leaders in Social Networks, the Delicious Case.pdf}
}

@incollection{lengRipplingEffectSocial2018,
  title = {The {{Rippling Effect}} of {{Social Influence}} via {{Phone Communication Network}}},
  booktitle = {Complex {{Spreading Phenomena}} in {{Social Systems}}},
  author = {Leng, Yan and Dong, Xiaowen and Moro, Esteban and Pentland, Alex ‘Sandy’},
  editor = {Lehmann, Sune and Ahn, Yong-Yeol},
  date = {2018},
  pages = {323--333},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-77332-2_17},
  url = {http://link.springer.com/10.1007/978-3-319-77332-2_17},
  urldate = {2024-03-18},
  isbn = {978-3-319-77331-5 978-3-319-77332-2},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/H6ZFBHJX/Leng et al_2018_The Rippling Effect of Social Influence via Phone Communication Network.pdf}
}

@manual{jaliliCentiserveFindGraph2017,
  type = {manual},
  title = {Centiserve: {{Find}} Graph Centrality Indices},
  author = {Jalili, Mahdi},
  date = {2017},
  url = {https://CRAN.R-project.org/package=centiserve}
}

@manual{csardiIgraphNetworkAnalysis2024,
  type = {manual},
  title = {{{igraph}}: {{Network}} Analysis and Visualization in {{R}}},
  author = {Csárdi, Gábor and Nepusz, Tamás and Traag, Vincent and Horvát, Szabolcs and Zanini, Fabio and Noom, Daniel and Müller, Kirill},
  date = {2024},
  doi = {10.5281/zenodo.7682609},
  url = {https://CRAN.R-project.org/package=igraph}
}

@article{palfreymanMarketsModelsMetrics2007,
  title = {Markets, Models and Metrics in Higher Education},
  author = {Palfreyman, David},
  date = {2007-07},
  journaltitle = {Perspectives: Policy and Practice in Higher Education},
  shortjournal = {Perspectives: Policy and Practice in Higher Education},
  volume = {11},
  number = {3},
  pages = {78--87},
  issn = {1360-3108, 1460-7018},
  doi = {10.1080/13603100701428320},
  url = {http://www.tandfonline.com/doi/abs/10.1080/13603100701428320},
  urldate = {2024-07-14},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/68MLB69X/Palfreyman_2007_Markets, models and metrics in higher education.pdf}
}

@article{gruberAcademicSelloutHow2014,
  title = {Academic Sell-out: How an Obsession with Metrics and Rankings Is Damaging Academia},
  shorttitle = {Academic Sell-Out},
  author = {Gruber, Thorsten},
  date = {2014-01-01},
  journaltitle = {Journal of Marketing for Higher Education},
  volume = {24},
  number = {2},
  pages = {165--177},
  publisher = {Loughborough University},
  issn = {0884-1241},
  doi = {10.1080/08841241.2014.970248},
  url = {https://repository.lboro.ac.uk/articles/journal_contribution/Academic_sell-out_how_an_obsession_with_metrics_and_rankings_is_damaging_academia/9503816/1},
  urldate = {2023-12-01},
  abstract = {Increasingly, academics have to demonstrate that their research has academic impact. Universities normally use journal rankings and journal impact factors to assess the research impact of individual academics. More recently, citation counts for individual articles and the h-index have also been used to measure the academic impact of academics. There are, however, several serious problems with relying on journal rankings, journal impact factors and citation counts. For example, articles without any impact may be published in highly ranked journals or journals with high impact factor, whereas articles with high impact could be published in lower ranked journals or journals with low impact factor. Citation counts can also be easily gamed and manipulated, and the h-index disadvantages early career academics. This paper discusses these and several other problems and suggests alternatives such as post-publication peer review and open-access journals.},
  langid = {english},
  file = {/Users/jfprice/Zotero/storage/HIGWF656/Gruber_2014_Academic sell-out.pdf}
}
